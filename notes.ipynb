{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8728889",
   "metadata": {},
   "source": [
    "## Building a container\n",
    "\n",
    "#### SSH into submit node (on local machine)\n",
    "\n",
    "- SSH into the submit node. For example, for my personal submit node, I do: `ssh pravindran@submit2.chtc.wisc.edu` and follow through the terminal messages.\n",
    "\n",
    "#### Initiate an interactive session (on CHTC submit node)\n",
    "\n",
    "- Create a submit file for starting an interactive session on a \"working\" machine. We will build our container using the interactive session on the working machine. \n",
    "\n",
    "- If the submit file is called `interactive_session_launcher.sub`, on the submit machine the command `condor_submit -i interactive_session_launcher.sub` launches an interactive session on the working machine. Note the working machine is arbitrarily assigned and changes everytime. So backup your creations in the interactive sessions before exiting the session.  \n",
    "\n",
    "- Contents of file: `interactive_session_launcher.sub` :\n",
    "```\n",
    "    # Submit file to use when launching an interactive session\n",
    "    universe = vanilla\n",
    "    log = interactive.log\n",
    "\n",
    "    #If your build job needs access to any files in your /home directory, \n",
    "    transfer them to your job using transfer_input_files\n",
    "    #transfer_input_files =\n",
    "    \n",
    "    +IsBuildJob = true\n",
    "    requirements = (OpSysMajorVer =?= 8)\n",
    "    request_cpus = 1\n",
    "    request_memory = 32GB\n",
    "    request_disk = 64GB\n",
    "\n",
    "    queue\n",
    "```\n",
    "\n",
    "- Note the following about the contents of `interactive_session_launcher.sub`:\n",
    "    - We have requested 32GB of memory, 64GB of disk, and 1 cpu. Insufficient resource requests will lead to memory errors when building the containers on the working machine. If something like this happens, investigate messages in the file `interactive.log`.\n",
    "\n",
    "- It all goes well, in a few seconds, you will be in an interactive session on the working machine.\n",
    "\n",
    "\n",
    "#### Build the container (on CHTC working node)\n",
    "- Create a definition file what we want in our container. \n",
    "\n",
    "- Contents of file: `pytch113_container.def` \n",
    "    ```\n",
    "    Bootstrap: docker\n",
    "    From: pytorch/pytorch:1.13.0-cuda11.6-cudnn8-devel\n",
    "\n",
    "    %post\n",
    "        conda install -c anaconda pandas\n",
    "        conda install -c anaconda scipy\n",
    "        conda install -c anaconda scikit-learn\n",
    "    ```\n",
    "    \n",
    "- Note the following about the contents of `pytch113_container.def`:\n",
    "    - The curated PyTorch 1.13.0 container is used.\n",
    "    - After that we install pandas, scipy, and scikit-learn. \n",
    "    \n",
    "- Submit a job that creates the container as follows: `apptainer build pytch113_container.sif pytch113_container.def`. If all goes well the file `pytch113_container.sif` will be created.\n",
    "\n",
    "- Move `pytch113_container.sif` to the `/staging/<username>`: `mv pytch113_container.sif /staging/<username>`\n",
    "\n",
    "- Exit from working node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba47303e",
   "metadata": {},
   "source": [
    "## Preparing for jobs submission\n",
    "\n",
    "\n",
    "#### Create the ingredients (on CHTC submit node)\n",
    "- The executable bash script that is run by every job with its own set of parameters. Here are contents for the file `executable_for_job.sh`:\n",
    "    ```\n",
    "    #!/bin/bash\n",
    "\n",
    "    DATAFILE=$1\n",
    "    PARAM=$2\n",
    "    OUTDIR=$3\n",
    "\n",
    "    python main.py --data_file $DATAFILE --out_dir $OUTDIR --param $PARAM\n",
    "    ```\n",
    "    \n",
    "- The executable expects 3 arguments when invoked. \n",
    "- ---\n",
    "- The `params_for_jobs.txt` contains values for the arguments expected by `executable_for_job.sh`. Here are the contents of `params_for_jobs.txt`:\n",
    "    ```\n",
    "    t1.pt, out, double\n",
    "    t1.pt, out, triple\n",
    "    t1.pt, out, quadruple\n",
    "    t2.pt, out, double\n",
    "    t2.pt, out, triple\n",
    "    t2.pt, out, quadruple\n",
    "    ```\n",
    "- ---\n",
    "- The jobs submission file (`jobs_submitter.sub`) takes the `executable_for_job.sh` and `params_for_jobs.txt` files and high-throughputs the computation into multiple jobs: it launches as the specified number of jobs and maps one job (an invocation of the `executable_for_job.sh` script with one setting for parameters from `params_for_jobs.txt`) to one machine on CHTC.\n",
    "\n",
    "- Contents of `jobs_submitter.sub`:\n",
    "    ```\n",
    "    # Submit jobs.\n",
    "\n",
    "    # Provide HTCondor with the name of your .sif file and universe information\n",
    "    # (`universe = container` is optional as long as `container_image` is specified)\n",
    "    container_image = pytch113_container.sif\n",
    "    universe = container\n",
    "\n",
    "    executable = executable_for_job.sh\n",
    "    arguments = $(DATAFILE) $(PARAM) $(OUTDIR)\n",
    "\n",
    "    # Tell HTCondor to transfer the my-container.sif file to each job\n",
    "    transfer_input_files = file:///staging/pravindran/pytch113_container.sif, main.py, params.py, data/$(DATAFILE)\n",
    "    transfer_output_files = $(OUTDIR)/$(DATAFILE)_X$(PARAM).pt\n",
    "    transfer_output_remaps = \"$(DATAFILE)_X$(PARAM).pt = out/$(DATAFILE)_X$(PARAM).pt\"\n",
    "\n",
    "    log = logs/$(CLUSTER).log\n",
    "    error = errors/$(CLUSTER)_$(PROCESS).err\n",
    "    output = outputs/$(CLUSTER)_$(PROCESS).out\n",
    "\n",
    "    # Make sure you request enough disk for the container image in addition to your other input files\n",
    "    request_cpus = 4\n",
    "    request_memory = 32GB\n",
    "    request_disk = 64GB      \n",
    "\n",
    "    queue DATAFILE, PARAM, OUTDIR from params_for_jobs.txt\n",
    "    ```\n",
    "    \n",
    "- Notes about contents of `jobs_submitter.sub`\n",
    "    - Expects jobs to be submitted from the directory in which `main.py` and `params.py` reside.\n",
    "    - Expects `pytch113_container.sif` to be in `/staging/pravindran/`.\n",
    "    - Put `executable_for_job.sh`, `params_for_jobs.txt`, and `jobs_submitter.sub` in the directory that contains `main.py` and `params.py`.\n",
    "    - The specification in `jobs_submitter.sub` will make the data file that is specified as input `data/$(DATAFILE)` to be copied as `$(DATAFILE)` in the same directory as `main.py` on the working node assigned to the job. So `main.py` must handle this. Also, our `main.py` creates a directory `out/` and puts the output file (say `job_output.txt`) inside it. CHTC copies `out/job_output.txt` into the same directory from which the jobs were submitted on the submit node. Hence if reorg of the transferred output data is required, we do an output remap in `transfer_output_remaps`. CHTC cannot create this `out/` and expects that directory `out/` exists in directory from which the jobs were submitted. We have to create the specified directory structure before submitting the jobs (see below). The path specified in `transfer_output_remaps` need not match those created by `main.py` and so `transfer_output_remaps` can be used to rename the files as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023e349",
   "metadata": {},
   "source": [
    "## High throughput processing\n",
    "\n",
    "#### Submitting the jobs (on CHTC submit node)\n",
    "\n",
    "- Clone this repo (`chtc_toy`) from GitHub: `git clone https://github.com/prabu-github/chtc_toy.git`.\n",
    "- Get into `chtc_toy` directory: `cd chtc_toy`.\n",
    "- Copy files into correct location: \n",
    "    - `cp chtc/executable_for_job.sh .`\n",
    "    - `cp chtc/params_for_jobs.txt .`\n",
    "    - `cp chtc/jobs_submitter.sub .`\n",
    "- Give permisssions: `chmod +777 executable_for_job.sh`.\n",
    "- Create the `out/` directory: `mkdir out`.\n",
    "- Submit: `condor_submit jobs_submitter.sub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c870909",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
